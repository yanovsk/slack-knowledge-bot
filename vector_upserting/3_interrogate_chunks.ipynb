{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import pinecone\n",
    "import json\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import time\n",
    "from json import JSONDecodeError\n",
    "import concurrent.futures\n",
    "import traceback\n",
    "import requests\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "from io import BytesIO\n",
    "\n",
    "#INSERT API KEYS HERE\n",
    "pinecone.init(api_key=\"\", environment=\"us-west4-gcp\")\n",
    "index = pinecone.Index(index_name='gpt-dashboard-db')\n",
    "os.environ['OPENAI_API_KEY'] = \"\"\n",
    "openai.api_key = \"\"\n",
    "\n",
    "embed_id = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 2: Interrogate\n",
    "from multiprocessing import Manager\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "manager = Manager()\n",
    "rate_limit_hit = manager.Value(bool, False)\n",
    "\n",
    "filenames = [] #<--- Add filenames with chunks to process here (e.g. 'channel.json')\n",
    "\n",
    "\n",
    "def get_qa(messages):\n",
    "    if rate_limit_hit.value:\n",
    "        print(\"Rate limit hit, sleeping for 60 seconds\")\n",
    "        time.sleep(60)\n",
    "        rate_limit_hit.value = False\n",
    "\n",
    "    user_input_gpt_gpt35 = \"\"\"#Instructions# You are an AI tool capable of scanning and interpreting communication  \\\n",
    "      threads on platforms such as Slack.  Your primary function is to identify key topics of discussion and create insightful \\\n",
    "        question and answer pairs based on these topics. The goal \\\n",
    "        is to generate around 15 Q&A pairs that could potentially be posed by Acme Inc. employees in relation to the subject \\\n",
    "        matter being discussed. However, should the thread contain a wider variety of topics, you may extend the number of Q&A pairs to 20. \\\n",
    "    #Input Format# You will be provided with a thread history, capturing interactions between employees asking and answering questions. \\\n",
    "        Analyze this thread and generate approximately 10 to 20 Q&A pairs, depending on the diversity of topics discussed. The pairs should\\\n",
    "        reference the original authors of the questions and answers where applicable.\n",
    "    #Example Output# Consider a scenario where the conversation thread focuses on cap table maintenance, with Branden asking questions \\\n",
    "    # and Jordan providing responses. An appropriate Q&A pair could be:\n",
    "    Q (Branden): Should we provide cap table maintenance for the customer?\n",
    "    A (Jordan): No, this is not...(continued)\n",
    "\n",
    "    #Thread history input data: # \"\"\" + messages\n",
    "\n",
    "    gpt35_system_prompt = \"You are helpful Q&A generating bot.\"\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt35_system_prompt}, {\"role\": \"user\", \"content\": user_input_gpt_gpt35}]\n",
    "\n",
    "\n",
    "    for i in range(6): \n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=messages,\n",
    "                max_tokens=900,\n",
    "                temperature=0.8\n",
    "            )\n",
    "            response_message = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "            prompt_tokens, completion_tokens = response['usage']['prompt_tokens'], response['usage']['completion_tokens']\n",
    "\n",
    "            return response_message, prompt_tokens, completion_tokens\n",
    "\n",
    "        except JSONDecodeError as e:\n",
    "            print(f'JSONDecodeError occurred: {e}')\n",
    "            time.sleep(10) \n",
    "            continue \n",
    "        except openai.error.ServiceUnavailableError as e:\n",
    "            print(f'Server error: {e}. Retrying in 10 seconds.')\n",
    "            time.sleep(10)  \n",
    "            continue\n",
    "        except openai.error.RateLimitError as e: \n",
    "            print(f'Hit rate limit. Waiting 60 sec')\n",
    "            rate_limit_hit.value = True\n",
    "            time.sleep(60)  \n",
    "            continue\n",
    "        except openai.error.APIError as e: \n",
    "            print(f'BAD GATEWAY ERROR. Waiting 20 sec')\n",
    "            rate_limit_hit.value = True\n",
    "            time.sleep(20)  \n",
    "            continue\n",
    "    raise TypeError(\"Function call not found in response after 5 retries\")\n",
    "\n",
    "\n",
    "def process_chunk(obj):\n",
    "    messages = obj[\"messages\"]\n",
    "    try:\n",
    "        qa, prompt_tokens, completion_tokens = get_qa(messages)\n",
    "        obj[\"qa\"] = qa\n",
    "        obj[\"prompt_tokens\"] = prompt_tokens\n",
    "        obj[\"completion_tokens\"] = completion_tokens\n",
    "    except openai.error.ServiceUnavailableError:\n",
    "        print('Server overload error occurred. Writing current data to JSON file.')\n",
    "        with open(\"error_file_chunks.json\", 'a') as f:\n",
    "            json.dump(obj, f)\n",
    "        traceback.print_exc()\n",
    "    except Exception as e:\n",
    "        with open(\"error_file_chunks.json\", 'a') as f:\n",
    "            json.dump(obj, f)\n",
    "        print(f'Unexpected error occurred: {e}')\n",
    "        traceback.print_exc()\n",
    "    return obj\n",
    "\n",
    "# Create a function to split the chunks into smaller groups, each with a token count <= 85000\n",
    "def split_chunks(chunks, max_token_count=30000):\n",
    "    grouped_chunks = []\n",
    "    current_group = []\n",
    "    current_group_token_count = 0\n",
    "\n",
    "    for chunk in chunks:\n",
    "        if current_group_token_count + chunk['token_count'] <= max_token_count:\n",
    "            current_group.append(chunk)\n",
    "            current_group_token_count += chunk['token_count']\n",
    "        else:\n",
    "            grouped_chunks.append(current_group)\n",
    "            current_group = [chunk]\n",
    "            current_group_token_count = chunk['token_count']\n",
    "\n",
    "    # Append the last group if it isn't empty\n",
    "    if current_group:\n",
    "        grouped_chunks.append(current_group)\n",
    "\n",
    "    return grouped_chunks\n",
    "\n",
    "\n",
    "#iterate through files with chunks from channels\n",
    "for filename in filenames:\n",
    "    with open(f\"./chunks_to_interrogate/{filename}\", 'r') as f:\n",
    "        chunks = json.load(f)\n",
    "\n",
    "    grouped_chunks = split_chunks(chunks)\n",
    "    len_of_chunks = [len(chunk) for chunk in grouped_chunks]\n",
    "    print(len_of_chunks, filename)\n",
    "\n",
    "    for i, group in enumerate(grouped_chunks):\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            new_chunks = list(executor.map(process_chunk, group))\n",
    "\n",
    "        # Open the output file in 'append' mode ('a') to add the new chunks to the end\n",
    "        with open(f'./to_pinecone/{filename}', 'a', encoding='utf-8') as f:\n",
    "            json.dump(new_chunks, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "        print(f\"Done. {i+1}/{len(grouped_chunks)},chunks: {len(group)}\")\n",
    "            \n",
    "        # Sleep for 1 minute after each group of chunks\n",
    "        if i != len(grouped_chunks) - 1:\n",
    "            time.sleep(65)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
